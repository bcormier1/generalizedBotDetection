{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "import datasets\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'overflowing_tokens': <tf.Tensor: shape=(1, 29), dtype=int32, numpy=\n",
      "array([[ 2335,    16, 11962,     6,     8,   269,  3829,     7, 36733,\n",
      "           15,     5,    92,  8774, 19357,  6993,    23,   204,   524,\n",
      "           25, 13543,    13,    77,    49,  2335,  8268,    24,    18,\n",
      "         1011,     4]], dtype=int32)>, 'num_truncated_tokens': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([29], dtype=int32)>, 'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[    0, 31414,     6,   127,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "{'overflowing_tokens': <tf.Tensor: shape=(1, 31), dtype=int32, numpy=\n",
      "array([[    6,   127,  2335,    16, 11962,     6,     8,   269,  3829,\n",
      "            7, 36733,    15,     5,    92,  8774, 19357,  6993,    23,\n",
      "          204,   524,    25, 13543,    13,    77,    49,  2335,  8268,\n",
      "           24,    18,  1011,     4]], dtype=int32)>, 'num_truncated_tokens': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([31], dtype=int32)>, 'input_ids': <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[    0, 31414,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[1, 1, 1]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute, and really likes to poop on the newighbors yard at 4 am as revenge for when their dog stole it's ball.\",\n",
    "                   max_length= 5, return_tensors=\"tf\", return_overflowing_tokens=True)\n",
    "print(inputs)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute, and really likes to poop on the newighbors yard at 4 am as revenge for when their dog stole it's ball.\",\n",
    "                   max_length= 3, return_tensors=\"tf\", return_overflowing_tokens=True)\n",
    "print(inputs)\n",
    "#outputs = model(inputs)\n",
    "\n",
    "#last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=\n",
       "array([[    0, 31414,     6,   127,  2335,    16, 11962,     2]],\n",
       "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wiki_set(data_set_type='train',n=10000):\n",
    "    overflow = ''\n",
    "    examples = []\n",
    "    for i in tqdm(range(n)):\n",
    "        buffer = overflow + wiki[data_set_type]['text'][i]\n",
    "        if len(buffer) >= MAX_LENGTH:\n",
    "            examples.append(buffer[:MAX_LENGTH])\n",
    "            overflow = ''\n",
    "        else:\n",
    "            overflow += wiki['train']['text'][i]\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:17<00:00,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Valkyria Chronicles III = \n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs paralle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wiki_examples = create_wiki_set(data_set_type='train',n=10)\n",
    "print(wiki_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(1, 8, 768), dtype=float32, numpy=\n",
       "array([[[-0.04778079,  0.08856517, -0.00979672, ..., -0.05444449,\n",
       "         -0.06716338, -0.00391623],\n",
       "        [-0.07121383,  0.01499833, -0.1298776 , ...,  0.06383334,\n",
       "          0.02963825, -0.08603133],\n",
       "        [ 0.09055017,  0.14373958,  0.08283181, ...,  0.05086066,\n",
       "         -0.03197741, -0.04901589],\n",
       "        ...,\n",
       "        [ 0.08531624,  0.21548253,  0.08490154, ..., -0.11496733,\n",
       "          0.03299987, -0.07903446],\n",
       "        [ 0.16793376,  0.128793  ,  0.00646019, ...,  0.03671372,\n",
       "         -0.06305141,  0.02762577],\n",
       "        [-0.04356978,  0.08920526, -0.03888651, ..., -0.09573355,\n",
       "         -0.07437792, -0.02838535]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
       "array([[-3.20835412e-03, -2.19398811e-01, -2.10875914e-01,\n",
       "        -7.67211840e-02,  1.20515734e-01,  2.04878107e-01,\n",
       "         2.60702759e-01, -8.43434557e-02, -7.25212172e-02,\n",
       "        -1.70245662e-01,  2.10894465e-01, -2.10080706e-02,\n",
       "        -8.20225626e-02,  1.01770028e-01, -1.44240707e-01,\n",
       "         4.92009878e-01,  2.12017640e-01, -4.57427651e-01,\n",
       "         3.59925367e-02, -1.54097788e-02, -2.72182375e-01,\n",
       "         8.27794075e-02,  4.70065057e-01,  3.35531026e-01,\n",
       "         1.15759946e-01,  6.07131459e-02, -1.33749053e-01,\n",
       "        -1.28289824e-02,  1.83979839e-01,  2.20602602e-01,\n",
       "         2.86060303e-01,  6.49519041e-02,  8.11276287e-02,\n",
       "         2.38027632e-01, -2.40373775e-01,  6.31177351e-02,\n",
       "        -3.10757160e-01,  2.31245067e-02,  2.60800332e-01,\n",
       "        -1.86507851e-01, -7.94521198e-02,  1.64428204e-01,\n",
       "         2.10449129e-01, -1.18400335e-01, -1.12902522e-01,\n",
       "         4.05719608e-01,  2.56426215e-01,  1.24019133e-02,\n",
       "        -1.40121520e-01, -9.03729349e-02, -3.54312032e-01,\n",
       "         3.36890906e-01,  2.84835756e-01,  1.95041850e-01,\n",
       "        -3.88983870e-03,  5.92864379e-02, -1.45375699e-01,\n",
       "         2.51302689e-01, -8.08212087e-02, -9.21375155e-02,\n",
       "        -1.17701449e-01, -2.02876329e-01, -1.32955713e-02,\n",
       "        -5.46009243e-02,  2.95995660e-02, -1.40465602e-01,\n",
       "         9.00239944e-02, -1.46244109e-01, -1.41902789e-01,\n",
       "         5.48128933e-02, -8.58305469e-02,  1.53002515e-01,\n",
       "         1.69136807e-01, -2.95962930e-01, -2.91939050e-01,\n",
       "         4.43661176e-02, -5.87140739e-01, -9.92226452e-02,\n",
       "         2.96371132e-01,  4.26589400e-01, -1.20493628e-01,\n",
       "         1.85383514e-01,  3.91205288e-02,  2.10799828e-01,\n",
       "        -1.10536814e-04, -4.03327532e-02, -3.02921757e-02,\n",
       "        -1.13737464e-01,  1.92575112e-01,  2.70846277e-01,\n",
       "        -1.98716417e-01, -3.77652884e-01,  6.66286573e-02,\n",
       "         1.19770812e-02, -9.29620638e-02,  2.19942592e-02,\n",
       "        -2.38061827e-02, -9.62426737e-02, -1.54129207e-01,\n",
       "        -1.67303458e-01,  5.45956753e-02, -2.69671708e-01,\n",
       "        -1.40773878e-01,  2.65931934e-01, -3.28430608e-02,\n",
       "        -1.97120741e-01, -7.78406672e-03,  3.11605662e-01,\n",
       "         7.45204464e-02, -1.16908878e-01, -1.90019906e-01,\n",
       "         4.27071571e-01,  3.08910906e-01,  1.99038535e-04,\n",
       "         3.09021841e-03,  1.75333872e-01,  1.32558957e-01,\n",
       "        -2.92308658e-01,  4.09372717e-01, -3.13201427e-01,\n",
       "        -1.78365782e-02, -9.42375660e-02,  1.11460842e-01,\n",
       "         1.59942210e-01, -2.21162051e-01,  2.85683036e-01,\n",
       "         1.37039304e-01,  2.68404692e-01,  1.80570111e-01,\n",
       "         1.06966279e-01, -2.95007527e-02,  1.45060733e-01,\n",
       "        -9.14728567e-02,  1.30431533e-01,  2.32660145e-01,\n",
       "         1.15183868e-01, -8.29361286e-03, -3.29770863e-01,\n",
       "        -2.05752045e-01,  2.69434959e-01,  3.41006100e-01,\n",
       "         1.58482626e-01, -5.10633253e-02,  1.89969853e-01,\n",
       "         8.57245475e-02,  2.21281230e-01,  1.43722787e-01,\n",
       "        -4.10222769e-01,  4.19181585e-02,  3.50817084e-01,\n",
       "         9.59414765e-02,  1.56078443e-01, -7.97151998e-02,\n",
       "        -2.80920953e-01, -2.64110923e-01, -8.83056894e-02,\n",
       "         5.00719175e-02, -3.28754365e-01, -1.32081404e-01,\n",
       "         3.65294904e-01,  2.14692689e-02, -9.13008396e-03,\n",
       "        -1.41113535e-01, -2.46367082e-01, -2.38322504e-02,\n",
       "        -1.23165026e-01,  1.06486222e-02,  1.10225715e-01,\n",
       "        -8.72697234e-02, -4.04573619e-01, -8.67537335e-02,\n",
       "        -5.57479680e-01, -1.05601937e-01,  1.71130121e-01,\n",
       "        -3.24255645e-01,  2.53680497e-01, -2.76805073e-01,\n",
       "         1.02617063e-01,  4.02728915e-01,  4.10414785e-02,\n",
       "        -6.04181597e-03, -1.88274935e-01, -1.89641565e-02,\n",
       "         9.24247950e-02,  3.31841797e-01,  2.45441973e-01,\n",
       "        -4.00647253e-01,  1.12104788e-01,  1.43804550e-01,\n",
       "         2.54461855e-01,  1.42715216e-01, -5.58097996e-02,\n",
       "        -1.28639355e-01,  1.56570539e-01, -2.00989634e-01,\n",
       "         1.79555342e-01, -2.21088782e-01,  1.80278227e-01,\n",
       "        -2.55239904e-01, -2.14609280e-01,  2.89018661e-01,\n",
       "        -3.99546087e-01, -2.03513447e-02,  8.74466076e-02,\n",
       "         2.68948913e-01,  1.29506178e-02, -3.03633306e-02,\n",
       "        -7.79506266e-02,  1.22122176e-01,  1.90515310e-01,\n",
       "         1.34314030e-01, -3.86831135e-01,  2.65822083e-01,\n",
       "        -2.70696189e-02, -2.07917485e-02, -2.93000471e-02,\n",
       "         1.62729949e-01,  2.45736971e-01,  9.68206972e-02,\n",
       "        -3.85696143e-01, -1.40931979e-01,  1.15571566e-01,\n",
       "         2.88035363e-01, -2.23671019e-01,  1.64160088e-01,\n",
       "        -2.81541824e-01, -3.90870959e-01, -1.50209859e-01,\n",
       "         2.10859701e-01,  2.31049985e-01,  1.68566480e-01,\n",
       "        -2.72874892e-01,  1.63064003e-01, -9.61965024e-02,\n",
       "        -4.29663271e-01, -3.66993457e-01, -1.07383884e-01,\n",
       "         2.50065655e-01,  1.72849178e-01,  1.88412786e-01,\n",
       "         2.37282708e-01,  3.97718400e-02,  1.20347865e-01,\n",
       "         1.44975558e-01,  1.55796409e-01, -1.50204033e-01,\n",
       "         1.90349638e-01, -3.56269479e-01, -5.85167445e-02,\n",
       "        -2.66237974e-01, -1.92025706e-01, -1.94656208e-01,\n",
       "         3.98378432e-01, -2.30641395e-01,  2.33293861e-01,\n",
       "         3.87280822e-01, -3.04069966e-01, -1.15069799e-01,\n",
       "         1.48588642e-01,  9.19808820e-02,  9.46981534e-02,\n",
       "        -1.17586061e-01,  1.95303455e-01,  1.48202315e-01,\n",
       "        -1.09415665e-01,  2.52500534e-01,  2.31210818e-03,\n",
       "         2.53496110e-01,  1.68595076e-01,  8.71359408e-02,\n",
       "         1.35134220e-01,  1.27087444e-01, -1.48624793e-01,\n",
       "         6.07507266e-02,  9.33445431e-03, -1.52348960e-02,\n",
       "        -2.31772855e-01, -1.58350185e-01,  2.31593430e-01,\n",
       "        -5.20966612e-02,  2.36924198e-02, -1.70812771e-01,\n",
       "        -1.12163700e-01,  2.90004518e-02,  4.03673619e-01,\n",
       "        -3.58097434e-01,  2.51180977e-01,  7.63967037e-02,\n",
       "         1.57716975e-01, -2.49080643e-01, -2.25926474e-01,\n",
       "         8.81317630e-02,  1.78568467e-01, -4.11153138e-01,\n",
       "         1.01238852e-02,  1.70343921e-01,  9.30642262e-02,\n",
       "         2.06053570e-01,  2.60113955e-01,  8.85624066e-03,\n",
       "        -1.15994424e-01,  4.90979046e-01, -1.61067083e-01,\n",
       "        -1.08544350e-01,  2.57025033e-01, -2.70816386e-01,\n",
       "        -2.79375583e-01,  2.48958617e-01, -2.87712514e-02,\n",
       "         2.99324363e-01,  1.26215771e-01,  5.40378913e-02,\n",
       "         7.49165565e-02, -6.00177467e-01,  6.10343181e-02,\n",
       "        -4.53015894e-01,  9.61085130e-03,  2.22184360e-02,\n",
       "        -8.25161338e-02, -1.98490947e-01,  1.48977339e-01,\n",
       "         2.96525747e-01, -2.56673664e-01, -2.87930351e-02,\n",
       "         1.88628480e-01,  6.92571178e-02, -1.23533025e-01,\n",
       "         4.75476474e-01, -1.03991870e-02,  2.11918890e-01,\n",
       "        -5.57040274e-02,  2.72053331e-01, -2.14468285e-01,\n",
       "         2.68447578e-01, -2.73713499e-01, -7.97929689e-02,\n",
       "         1.96780413e-02,  7.93839917e-02,  6.90158308e-02,\n",
       "        -6.24677986e-02, -3.39709640e-01,  2.35161781e-01,\n",
       "        -7.93724880e-03, -5.41508980e-02, -3.69961224e-02,\n",
       "         1.02098070e-01,  8.38435953e-04,  4.83319610e-02,\n",
       "         5.84419630e-02,  3.12681675e-01,  2.29631141e-01,\n",
       "        -1.59875192e-02, -3.71129543e-01, -2.71662269e-02,\n",
       "        -8.35823417e-02,  3.86972688e-02,  4.89381179e-02,\n",
       "        -1.57828685e-02,  4.39692676e-01, -8.11677799e-02,\n",
       "         3.53093352e-03, -1.42843679e-01,  2.61004537e-01,\n",
       "         2.11246416e-01,  1.26272008e-01,  1.21504202e-01,\n",
       "         5.61257116e-02,  1.27897829e-01, -5.34672067e-02,\n",
       "        -7.77304498e-03, -1.56708181e-01, -2.33479589e-01,\n",
       "        -2.75325239e-01,  2.08815768e-01, -2.39050195e-01,\n",
       "        -1.64440319e-01,  1.60898685e-01,  2.13066816e-01,\n",
       "        -1.54470757e-01,  1.44508615e-01,  3.04993361e-01,\n",
       "         1.03659652e-01, -1.48999706e-01,  2.66285807e-01,\n",
       "        -1.08612746e-01,  9.78242457e-02,  3.17536443e-01,\n",
       "        -1.55099155e-02,  1.78095445e-01,  4.90861505e-01,\n",
       "         2.11000264e-01, -3.69210333e-01, -3.94604206e-02,\n",
       "        -2.12497726e-01, -2.95321876e-03,  2.37470895e-01,\n",
       "        -1.47297293e-01,  2.02547729e-01,  3.85129422e-01,\n",
       "         3.06374103e-01,  4.56059247e-01,  8.66976660e-03,\n",
       "        -1.33133337e-01,  8.31007659e-02,  2.19721362e-01,\n",
       "         3.52087133e-02, -1.52064025e-01, -1.82708472e-01,\n",
       "         2.53969640e-01,  5.98566644e-02, -1.41932696e-01,\n",
       "        -3.83067653e-02, -1.45527452e-01,  4.71653305e-02,\n",
       "        -1.33252501e-01, -3.94326091e-01,  4.24360037e-02,\n",
       "         1.94291025e-01, -4.80013311e-01,  8.42619762e-02,\n",
       "        -2.76828051e-01,  3.64395678e-02, -2.34952360e-01,\n",
       "         2.10428059e-01, -2.14833766e-01, -1.16945967e-01,\n",
       "         3.99198383e-01, -8.22381377e-02,  5.33655770e-02,\n",
       "        -1.81993872e-01, -1.42749161e-01,  2.00184714e-02,\n",
       "         1.15067950e-02, -3.60208303e-02, -2.61897221e-02,\n",
       "         3.34181428e-01, -1.30556464e-01,  3.29882130e-02,\n",
       "         2.24990733e-02,  2.07253948e-01, -3.88171002e-02,\n",
       "         1.98677108e-01,  2.05992144e-02, -1.32148027e-01,\n",
       "        -3.74775290e-01,  1.37686551e-01, -1.99719638e-01,\n",
       "        -4.21547860e-01, -3.75748485e-01,  3.54461610e-01,\n",
       "        -1.21602431e-01, -2.48048946e-01, -2.11665630e-01,\n",
       "        -2.57440507e-01,  7.95759931e-02,  1.77274376e-01,\n",
       "         4.61063057e-01, -3.83631557e-01, -8.99177715e-02,\n",
       "         4.67975348e-01, -6.33113384e-02, -1.83109462e-01,\n",
       "         2.96509862e-01,  1.80378720e-01, -3.35837752e-01,\n",
       "         3.33576500e-01,  2.59956956e-01, -4.43896949e-02,\n",
       "         2.14323141e-02,  5.14500678e-01,  1.27648681e-01,\n",
       "         2.09132195e-01, -2.31019288e-01,  4.49822962e-01,\n",
       "        -2.21964628e-01,  3.20710182e-01, -1.43692017e-01,\n",
       "        -2.13017955e-01, -2.24353179e-01, -5.35157137e-03,\n",
       "         3.32329512e-01,  1.80818453e-01, -4.20105398e-01,\n",
       "        -1.12855799e-01,  4.20588441e-02,  3.38003486e-01,\n",
       "        -3.84709299e-01, -9.51941609e-02,  1.41457664e-02,\n",
       "        -3.56040627e-01,  1.03133976e-01,  9.50982496e-02,\n",
       "         2.25736484e-01, -3.78652871e-01,  3.62465973e-03,\n",
       "         4.05021966e-01, -3.09433401e-01,  1.25502035e-01,\n",
       "         3.03708702e-01,  8.10050368e-02,  3.37606847e-01,\n",
       "        -4.38283682e-02, -3.55926086e-03,  4.55769785e-02,\n",
       "        -2.23708481e-01, -3.73146273e-02,  1.38598427e-01,\n",
       "         5.53459108e-01,  1.48177356e-01, -3.81702840e-01,\n",
       "         8.53189453e-02,  2.43814170e-01, -1.77003711e-01,\n",
       "         3.03410470e-01, -8.32461640e-02, -5.37493490e-02,\n",
       "         2.68746108e-01, -4.74042930e-02,  1.45662785e-01,\n",
       "        -8.51082280e-02, -2.34766662e-01, -3.02374333e-01,\n",
       "         3.72062206e-01, -2.14252189e-01, -1.13997847e-01,\n",
       "        -1.58389300e-01, -1.17158659e-01, -1.41414911e-01,\n",
       "         3.60584110e-02, -3.95093739e-01,  3.37784857e-01,\n",
       "         1.23257890e-01, -2.11187914e-01, -9.71143320e-02,\n",
       "        -9.19383392e-02, -1.63676471e-01, -2.25340128e-01,\n",
       "        -2.61481255e-01,  4.41721827e-01, -1.58480570e-01,\n",
       "        -4.50320154e-01,  2.57696241e-01,  2.48039942e-02,\n",
       "         3.40881974e-01,  3.57584357e-02,  1.01790577e-01,\n",
       "        -4.35351878e-02,  1.30057111e-01,  1.02018915e-01,\n",
       "        -1.23192519e-01,  2.68384248e-01,  5.63500710e-02,\n",
       "        -5.59554696e-01, -1.21658668e-01, -2.08712190e-01,\n",
       "         7.01977313e-02,  1.92833558e-01, -3.40267181e-01,\n",
       "         1.83589533e-02,  3.04272007e-02,  1.41745582e-01,\n",
       "         2.05197092e-02, -1.10924438e-01, -7.23680779e-02,\n",
       "         4.05556351e-01,  2.38585278e-01,  2.84832627e-01,\n",
       "         9.04108435e-02,  2.41680309e-01, -1.55831929e-02,\n",
       "        -3.35555732e-01,  3.57194915e-02,  8.47159475e-02,\n",
       "        -1.99511960e-01,  4.24500048e-01, -1.03254683e-01,\n",
       "        -3.91195238e-01, -6.81239143e-02,  3.96967381e-01,\n",
       "         1.10351965e-01, -2.60562487e-02, -4.74279560e-02,\n",
       "         2.01513410e-01,  1.60932779e-01, -1.37656942e-01,\n",
       "         1.90950975e-01, -4.21219207e-02, -1.41722724e-01,\n",
       "        -1.16266742e-01,  8.01678747e-02, -2.17977569e-01,\n",
       "         4.42189425e-02, -1.44717634e-01, -9.56381392e-03,\n",
       "        -2.00814664e-01,  7.95970019e-03, -1.90851524e-01,\n",
       "         2.55983353e-01, -3.37126285e-01,  1.14069864e-01,\n",
       "         7.07078576e-02,  2.94569165e-01, -3.43978435e-01,\n",
       "        -1.65677950e-01, -6.88137636e-02,  1.56920895e-01,\n",
       "         2.68606544e-01,  3.48458529e-01,  2.13631205e-02,\n",
       "         1.21829538e-02, -1.54401094e-01, -2.58027613e-01,\n",
       "         7.18437880e-02, -1.93896934e-01,  1.39839128e-01,\n",
       "         7.08630383e-02,  2.62113750e-01, -3.03361446e-01,\n",
       "        -1.88471347e-01,  2.22629875e-01, -1.07453465e-01,\n",
       "        -1.54276013e-01,  4.08734679e-01,  2.41009220e-01,\n",
       "         2.14567006e-01,  2.52576545e-02,  2.47452497e-01,\n",
       "         3.04050408e-02, -1.74306452e-01, -1.26085475e-01,\n",
       "        -2.50280708e-01,  7.94503540e-02, -7.41636828e-02,\n",
       "        -5.30130267e-02, -6.41489774e-02, -1.04493357e-01,\n",
       "        -2.06506178e-01, -1.66208416e-01,  1.38979912e-01,\n",
       "         1.46125808e-01,  3.67618129e-02, -5.05397208e-02,\n",
       "        -2.27870047e-02, -2.76609540e-01,  2.96194643e-01,\n",
       "         2.02105418e-02,  7.43673816e-02, -6.51659444e-02,\n",
       "         2.90482864e-02, -1.45913571e-01,  2.40871921e-01,\n",
       "         2.10742623e-01,  9.09499675e-02, -1.99213982e-01,\n",
       "        -4.87106591e-02, -2.82545775e-01, -3.61319721e-01,\n",
       "         6.19724058e-02,  1.38653949e-01,  1.05568230e-01,\n",
       "        -6.88275918e-02, -2.82527655e-01, -1.02485577e-02,\n",
       "        -1.45787120e-01,  1.74985990e-01,  1.53910546e-02,\n",
       "        -1.53217718e-01, -8.27573389e-02, -5.33861592e-02,\n",
       "        -5.51173463e-02,  7.97339603e-02, -2.17710793e-01,\n",
       "        -1.88134775e-01, -1.18729241e-01, -6.53318986e-02,\n",
       "        -7.35756531e-02,  3.53047729e-01, -4.66192439e-02,\n",
       "         2.82075197e-01, -1.48586452e-01,  1.07349409e-03,\n",
       "        -1.66373372e-01,  1.13654107e-01, -6.27936423e-02,\n",
       "         6.21112436e-02,  2.78822601e-01, -4.40709680e-01,\n",
       "        -1.60827056e-01, -2.96949618e-03, -2.13256031e-01,\n",
       "        -1.65362239e-01, -6.15230538e-02, -3.72082889e-02,\n",
       "         2.30685785e-01, -3.40577304e-01,  2.32357278e-01,\n",
       "        -1.20225221e-01,  1.92554653e-01, -7.20600486e-02,\n",
       "        -2.55192578e-01, -1.59226045e-01,  1.68811847e-02,\n",
       "         2.47110665e-01, -3.47247034e-01, -2.32618764e-01,\n",
       "        -2.77012408e-01, -1.03173070e-01, -9.32721794e-02,\n",
       "        -2.60390788e-01,  4.25467372e-01, -1.07975043e-01,\n",
       "        -7.44473487e-02,  1.90670807e-02,  4.43330705e-01,\n",
       "         2.06673980e-01,  1.46707669e-01,  2.12718159e-01,\n",
       "        -1.55562721e-02,  2.98302602e-02,  1.13627359e-01,\n",
       "        -4.58241671e-01,  2.29588926e-01, -2.31054947e-01,\n",
       "        -1.19013317e-01, -2.55083456e-03,  8.79378170e-02,\n",
       "        -2.85784509e-02,  1.00002550e-02, -1.33525833e-01,\n",
       "        -1.12019874e-01,  2.16943413e-01, -3.68301898e-01,\n",
       "        -3.18599269e-02,  2.55869031e-01,  1.54189110e-01,\n",
       "        -2.50107050e-01,  4.01220359e-02,  1.13599047e-01,\n",
       "         3.76821220e-01,  1.00478001e-01, -2.25016668e-01,\n",
       "         1.27526373e-01, -3.43337715e-01, -4.19576950e-02,\n",
       "        -1.91375822e-01, -2.92332470e-01,  1.55397221e-01,\n",
       "        -6.11723177e-02,  7.52292797e-02, -8.88148770e-02,\n",
       "        -2.94407517e-01,  2.15898022e-01, -5.99371791e-02,\n",
       "        -7.05182180e-02,  4.17251319e-01,  3.51346359e-02,\n",
       "        -1.30387709e-01,  1.53218076e-01,  6.31484343e-03,\n",
       "         7.28415698e-03, -1.01523824e-01,  2.61933208e-01,\n",
       "         2.16599420e-01, -2.74035037e-01,  1.44592419e-01,\n",
       "        -1.34544015e-01, -3.17012854e-02, -1.12099282e-01]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_roberta(learning_rate = 0.00001,\n",
    "                      dropout=0.3,\n",
    "                      hidden_size=100):\n",
    "    \n",
    "    model = TFRobertaModel.from_pretrained(\"roberta-base\")\n",
    "    \n",
    "    input_ids = tf.keras.layers.Input(shape=(100,), dtype=tf.int64, name='input_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(100,), dtype=tf.int64, name='attention_mask_layer')\n",
    "    \n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                   'attention_mask': attention_mask}         \n",
    "\n",
    "    bert_out = model(bert_inputs)[1]\n",
    "    \n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(bert_out)\n",
    "    dropped = tf.keras.layers.Dropout(dropout)(hidden)  \n",
    "\n",
    "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(dropped)\n",
    "\n",
    "    \n",
    "    detection_model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[classification])\n",
    "    \n",
    "    detection_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                            loss='binary_crossentropy', \n",
    "                            metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()]) \n",
    "    return detection_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/home/brody/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1036.57it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki = datasets.load_dataset(\"wikitext\",'wikitext-103-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/brody/school/w266/data/generated_wiki_text_10k.data', 'rb') as f:\n",
    "    generated_texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_texts[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". At the height of his popularity in 1882 he became one of Europe's leading entertainers. He became one of Europe's most well @-@ known musicians, selling over 10 @,@ 000 @,@ 000 albums as of 2015. He is widely regarded as one of Europe's most successful musicians, receiving accolades such as the Royal Philharmonic Lifetime Achievement Award in 2002. He was inducted into the Grammy Hall of Fame in 1988, and continues to perform at the Academy of Motion Picture Arts and Sciences every year thereafter, and has been featured on Rolling Stone's list of America's 100 Greatest Musicians of All @-@ Decade. \n",
      " = = Early life = = \n",
      " John Lewis Lewis was born on March 3, 1872, the youngest son of James Lewis and Margaret Alice Lewis. He grew up in New Haven, New Haven, Connecticut, where he moved with his mother and elder brother to Providence, Rhode Island when he was four years of age. Shortly thereafter, his parents emigrated to the West Indies, where it was rumoured that the Lewis family was descended from a Native American tribesman, but this never occurred. When Lewis was ten years old, his father\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "'s success in North America. \n",
      " = = Reception = = \n",
      " = = Track listing = = \n",
      " All songs written and co @-@ written by Jon Bon Jovi. \n",
      " = = Charts = = \n",
      " = = Certifications = = \n",
      " = M @-@ 53 ( Michigan highway ) = \n",
      " M @-@ 53 is a state trunkline state trunkline highway in the U.S. state of Michigan that originates just east @-@ southeast of Kalamazoo. It runs 1 1 ⁄ 2 @-@ mile ( 0 @.@ 62 km ) northward from an intersection with I @-@ 495 ( Michigan Avenue ) to I @-@ 695's southern terminus. The western terminus is I @-@ 495, which terminates at M @-@ 53's northern terminus. \n",
      " = = Route description = = \n",
      " M @-@ 53 begins just east @-@ southeast of I @-@ 495's southern terminus at an I @-@ 495 interchange. Northbound M @-@ 53 merges I @-@ 495's western terminus and\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      " was released. \n",
      " = = Gameplay = = \n",
      " Grand Theft Auto : San Andreas is a turn @-@ and @-@ a @-@ half @-@ hour shoot'em @-@ up in which the player assumes the role of Samus Aran. Samus acquires power @-@ ups scattered throughout the game's environments in order to progress through the game's campaign. These power @-@ ups vary depending on which power @-@ up Samus wields, and Samus is equipped with special power @-@ ups in addition to standard power @-@ ups. For example, Samus acquires a power @-@ up similar to Gung @-@ Ho's power @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      ", \" and \" The Man With The Golden Claws \". On February 2, 2012, Beyoncé performed \" Don 't Stop Believin'\" on The Ellen DeGeneres Show. \n",
      " = = Formats and track listings = = \n",
      " = = Charts and certifications = = \n",
      " Notes \n",
      " ^ a signifies a co @-@ producer \n",
      " ^ b signifies an additional co @-@ producer \n",
      " ^ c signifies an additional producer \n",
      " ^ d signifies an additional producer \n",
      " ^ e signifies an additional producer \n",
      " ^ f signifies an additional producer \n",
      " ^ g signifies an additional producer \n",
      " ^ h signifies an additional producer \n",
      " ^ i signifies an additional producer \n",
      " ^ j signifies an additional producer \n",
      " ^ k signifies an additional producer \n",
      " ^ l signifies an additional producer \n",
      " ^ m signifies an additional producer \n",
      " ^ o signifies an additional producer \n",
      " ^ p signifies an additional producer \n",
      " ^ q signifies an additional producer \n",
      " ^ r signifies an additional producer \n",
      " ^ s signifies an additional producer \n",
      " ^ t signifies an additional producer \n",
      " ^ u signifies an additional producer \n",
      " ^ v signifies an additional producer \n",
      " ^ w signifies an additional producer \n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      " that he had been unable to portray onscreen during the filming process, she said it was a \" dream of mine \" to be involved in Disney's live @-@ action live action adventure films. She elaborated : \" I just thought,'It's kind of cool. It's like I'm making a movie out there and I'm happy I made it. I didn ’ t know what I was supposed to do.'I wasn ’ t able to tell them where I was supposed to go with it. So I kind of decided to go home and look at the film as if it couldn ’ t happen. It's great to be able to work with them again because they ’ re kind of my dad ’ s family, so there ’ s going to be a lot of laughter out there. \" \n",
      " = = Reception = = \n",
      " = = = Box @-@ office = = = \n",
      " Toy Story 3 opened its North American debut on November 11, 2006, grossing US $ 29 @.@ 5 million in 3 @,@ 495 theaters in its first @-@ weekend box @-@ office performance. \n",
      " Toy Story 3 was\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      ", as well @-@ known examples exist such as those developed for use in nuclear propulsion. \n",
      " = = History = = \n",
      " = = = Early history = = = \n",
      " It is unclear exactly when the development of the nuclear @-@ powered electric locomotives began, or who is known to have been involved in the development and use of them. The first electric locomotives were produced between 1819 and 1824. By the end of World War I, it was apparent that electric locomotives were becoming more widely available, allowing them to be operated more efficiently than coal @-@ fired locomotives such as that of locomotives belonging to different classes. \n",
      " Electric locomotives were introduced into locomotives starting in 1819. It is possible that locomotives were designed and manufactured primarily for use in locomotives designed for use in coal @-@ fired locomotives. \n",
      " = = Development = = \n",
      " = = = Background = = = \n",
      " During the Napoleonic Wars in 1814 – 1815, steam locomotives were used to transport troops as part of France's campaign in the Seven Years'War against the Kingdom of Great Britain. It is\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      ". Since then, it has been used as a counter @-@ tool in attacks on military targets, such as air bases, dams, mines and oil facilities. \n",
      " = = Taxonomy = = \n",
      " The generic name Collybia comes from the Greek words phallus ( σοῦφόνος ) meaning \" plough @-@ and @- @-@ thorn @-@ spike @-@ plough \", \" plough @-@ and @-@ twig @-@ tree \" and σουλέούῦνούῦς meaning plough @-@ and @-@ twig @-@ tree. \n",
      " Collybia is most commonly known for its plough @-@ and @-@ twig @-@ tree appearance ; it can also be distinguished microscopically from other Collybia species, such as those found in Europe and Australia, as well as those found in Asia and North America, as illustrated below. \n",
      " = = Distribution and habitat = = \n",
      " Collybia can be found throughout North @-@ East Asia, Central America,\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      " = = Background = = \n",
      " In December 2012, the U.S Department of Homeland Security ( Homeland Security Administration ) released a memorandum of understanding ( Memorandum of Understanding ) for the implementation of President Barack Obama ’ s Homeland Security Reinvestment and Reinvestment Program ( H.R. 2782 ), which aims \" to enhance the safety and well @-@ being of U.S. citizens living in low @-@ security areas \", among other initiatives. On June 1, 2013, President @-@ in @-@ Chief John F. Kennedy issued Executive Order 1126, directing Immigration and Naturalization Service ( INS @-@ INS @-@ REV ) to work to enhance Homeland Security Reinvestment Program ( H.R @-@ 2782 ). According to the U.S. Department of Homeland @-@ Insular Affairs, H.R @-@ 2782 was designed to increase security for residents of low @-@ security @-@ designated U.S @-@ flagged countries such as Honduras, El Salvador, Guatemala, El Salvador @-@ Panama, El Salvador @-@ Belize @-@ Guatemala @-@ Nicaragua @-@ Belize @-\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      ". This has to do with two factors. First of all, there may not be enough fat soluble lipids in the blood to compensate for weight loss in athletes who have diabetes. Secondly there is not enough fat soluble lipids in the bloodstream to compensate for weight loss in athletes who have diabetes who do not have Type 2 diabetes. As a result, there may not even be enough saturated fatty acids in the bloodstream to compensate for weight loss in athletes who have Type 2 diabetes. \n",
      " There is some debate as to what is wrong with Type @-@ 2 diabetes. \n",
      " = = Epidemiology = = \n",
      " According to epidemiologic studies, there have been over 3 @,@ 000 documented cases of Type @-@ 2 diabetes in the U.S. According to the Centers for Disease Control and Prevention ( CDC ), there have been 2 @,@ 062 hospitalizations per 1 @,@ 000 people as of 2011 for Type @-@ 2 diabetics. There have also been 18 hospitalizations per 1 @,@ 000 people as of 2011 for Type @-@ 2 diabetics. According to the World Health Organization ( World Health Organization ) there have been 2 @,@ 516 traffic accidents in\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      " = = = \n",
      " On February 21, 2007, it was announced that the band had entered talks about a recording deal with Polydor Records, one of their record label's parent companies. On February 27, it was reported that Bon Jovi's record deal was extended by six months. On March 9, it was reported that Bon Jovi's recording contract was due to expire on July 31, 2007, at the age of 33 years old. Bon Jovi's record deal with Polydor would not be renewed for another six months. Bon Jovi's official Facebook page states, “ We look forward to seeing what they ’ re up to ”, adding : \n",
      " It ’ ’ s amazing to work with them because they have so many different things to work on. It ’ s been such an honor to work with them. It ’ s been my dream come true to work with them. It ’ s great working with them and I think it ’ s exciting. ” \n",
      " On April 24, Bon Jovi announced that Bon Jovi @-@ produced songs from Bon Jovi's upcoming ninth studio album would be released on April 28, 2007.\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in generated_texts[10:20]:\n",
    "    print(i)\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_gen = []\n",
    "for i in generated_texts:\n",
    "    split_gen.extend(i.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_gen = []\n",
    "for example in generated_texts:\n",
    "    if len(example) >= MAX_LENGTH:\n",
    "        cleaned_gen.append(example[:MAX_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9900\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_gen))\n",
    "random.shuffle(cleaned_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = = = \n",
      " In 1846, an Act of Parliament passed by the English House of Burgesses was passed by the King of Great @-@ Great @-@ Yarmouth, Abbot of Lichfield, to establish a monastic parish church on Lichfield Road in Lichfield's Borough of Lichfield. At that time it consisted exclusively of abbeys, abbots who were to preside over congregations in England and Wales at that time. Abbeys were said to act as chaplains at monasteries throughout Great @-@ Great @-@ Yarmouth's Borough of Lichfield. Abbot\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 15000/15000 [7:30:37<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "cleaned_wiki_train = create_wiki_set(data_set_type='train',n=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4752\n",
      "[' = Valkyria Chronicles III = \\n Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs paralle', \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening them\", \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise wit\", ' = = Gameplay = = \\n As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that ']\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_wiki_train))\n",
    "print(cleaned_wiki_train[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n"
     ]
    }
   ],
   "source": [
    "'''cleaned_wiki = []\n",
    "i = 0\n",
    "while len(cleaned_wiki) < len(cleaned_gen):\n",
    "    example = wiki['train']['text'][i]\n",
    "    if len(example) >= MAX_LENGTH:\n",
    "        cleaned_wiki.append(example[:MAX_LENGTH])\n",
    "    i+=1\n",
    "    if i %100 == 0:\n",
    "        print(i)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4752"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/brody/school/w266/data/cleaned_generated_text_wn.data', 'wb') as f:\n",
    "    pickle.dump(cleaned_gen, f)\n",
    "with open('/home/brody/school/w266/data/cleaned_wiki_text_wn.data', 'wb') as f:\n",
    "    pickle.dump(cleaned_wiki_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4752\n"
     ]
    }
   ],
   "source": [
    "cleaned_wiki = cleaned_wiki_train[:len(cleaned_gen)]\n",
    "print(len(cleaned_wiki))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 for human, 1 for bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train_labels = np.array([0]*(len(cleaned_wiki)-500) + [1]*(len(cleaned_gen)-500))\n",
    "wiki_val_labels = np.array([0]*500+[1]*500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = cleaned_wiki[:-500] + cleaned_gen[:-500]\n",
    "val_texts = cleaned_wiki[-500:] + cleaned_gen[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=100, return_tensors='tf')\n",
    "valid_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=100, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{250}\n",
      "{'input_ids': <tf.Tensor: shape=(2084, 100), dtype=int32, numpy=\n",
      "array([[   0, 2211,  267, ...,    1,    1,    1],\n",
      "       [   0,   20,  177, ...,    1,    1,    1],\n",
      "       [   0,   85, 1145, ...,    1,    1,    1],\n",
      "       ...,\n",
      "       [   0,   96,    5, ...,    1,    1,    1],\n",
      "       [   0,  374,  883, ...,    1,    1,    1],\n",
      "       [   0,   96,   63, ...,    1,    1,    1]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2084, 100), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "lengths = set()\n",
    "for i in val_texts:\n",
    "    lengths.add(len(i))\n",
    "print(lengths)\n",
    "print(train_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " attention_mask_layer (InputLay  [(None, 100)]       0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_ids_layer (InputLayer)   [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['attention_mask_layer[0][0]',   \n",
      " el)                            thPoolingAndCrossAt               'input_ids_layer[0][0]']        \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 100,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " hidden_layer (Dense)           (None, 100)          76900       ['tf_roberta_model[0][1]']       \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 100)          0           ['hidden_layer[0][0]']           \n",
      "                                                                                                  \n",
      " classification_layer (Dense)   (None, 1)            101         ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124,722,633\n",
      "Trainable params: 124,722,633\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model = build_base_roberta()\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1707/1707 [==============================] - 194s 109ms/step - loss: 0.0237 - accuracy: 0.9895 - precision_2: 0.9874 - recall_2: 0.9974 - val_loss: 0.0013 - val_accuracy: 0.9990 - val_precision_2: 1.0000 - val_recall_2: 0.9980\n",
      "Epoch 2/3\n",
      "1707/1707 [==============================] - 187s 110ms/step - loss: 0.0025 - accuracy: 0.9996 - precision_2: 0.9995 - recall_2: 0.9999 - val_loss: 4.8692e-04 - val_accuracy: 1.0000 - val_precision_2: 1.0000 - val_recall_2: 1.0000\n",
      "Epoch 3/3\n",
      "1707/1707 [==============================] - 187s 110ms/step - loss: 0.0025 - accuracy: 0.9994 - precision_2: 0.9993 - recall_2: 0.9999 - val_loss: 0.0043 - val_accuracy: 0.9990 - val_precision_2: 0.9980 - val_recall_2: 1.0000\n"
     ]
    }
   ],
   "source": [
    "bert_model = build_base_roberta(learning_rate = 0.00001,\n",
    "                      dropout=0.3,\n",
    "                      hidden_size=300)\n",
    "bert_model_history = bert_model.fit([train_encodings.input_ids, train_encodings.attention_mask], \n",
    "                                                  wiki_train_labels,   \n",
    "                                                  validation_data=([valid_encodings.input_ids, valid_encodings.attention_mask], \n",
    "                                                  wiki_val_labels),    \n",
    "                                                  batch_size=8, \n",
    "                                                  epochs=3, shuffle=True,)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(axs, history1, \n",
    "              y_lim_loss_lower=0.4, \n",
    "              y_lim_loss_upper=0.6,\n",
    "              y_lim_accuracy_lower=0.7, \n",
    "              y_lim_accuracy_upper=0.8,\n",
    "              model_1_name='model 1',\n",
    "              \n",
    "             ):\n",
    "    box = dict(facecolor='yellow', pad=5, alpha=0.2)\n",
    "\n",
    "    ax1 = axs[0]\n",
    "    ax1.plot(history1.history['loss'])\n",
    "    ax1.plot(history1.history['val_loss'])\n",
    "    ax1.set_title('loss - ' + model_1_name)\n",
    "    ax1.set_ylabel('loss', bbox=box)\n",
    "    ax1.set_ylim(y_lim_loss_lower, y_lim_loss_upper)\n",
    "\n",
    "    ax3 = axs[1]\n",
    "    ax3.set_title('accuracy - ' + model_1_name)\n",
    "    ax3.plot(history1.history['accuracy'])\n",
    "    ax3.plot(history1.history['val_accuracy'])\n",
    "    ax3.set_ylabel('accuracy', bbox=box)\n",
    "    ax3.set_ylim(y_lim_accuracy_lower, y_lim_accuracy_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m fig\u001b[38;5;241m.\u001b[39msubplots_adjust(left\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, wspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m)\n\u001b[1;32m      3\u001b[0m make_plot(axs, \n\u001b[1;32m      4\u001b[0m           bert_model_history,\n\u001b[1;32m      5\u001b[0m          y_lim_accuracy_lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.40\u001b[39m,\n\u001b[1;32m      6\u001b[0m          y_lim_accuracy_upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.82\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m fig\u001b[38;5;241m.\u001b[39malign_ylabels(\u001b[43maxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      9\u001b[0m fig\u001b[38;5;241m.\u001b[39mset_size_inches(\u001b[38;5;241m18.5\u001b[39m, \u001b[38;5;241m10.5\u001b[39m)\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "fig.subplots_adjust(left=0.2, wspace=0.6)\n",
    "make_plot(axs, \n",
    "          bert_model_history,\n",
    "         y_lim_accuracy_lower=0.40,\n",
    "         y_lim_accuracy_upper=0.82)\n",
    "\n",
    "fig.align_ylabels(axs[:, 1])\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses, embeddings_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0c6e63bf-dd96-46e6-a842-1065152c05f5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0c6e63bf-dd96-46e6-a842-1065152c05f5/assets\n"
     ]
    }
   ],
   "source": [
    "with open('/home/brody/school/w266/baseline_roberta/best_baseline.data', 'wb') as f:\n",
    "    pickle.dump(bert_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses, embeddings_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/brody/school/w266/baseline_roberta/keras_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/brody/school/w266/baseline_roberta/keras_model/assets\n"
     ]
    }
   ],
   "source": [
    "bert_model.save('/home/brody/school/w266/baseline_roberta/keras_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweep Fake data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweep_val = pd.read_csv('/home/brody/school/w266/tweep_fake/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "      <th>class_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ahadsheriff</td>\n",
       "      <td>TIGHT, TIGHT, TIGHT, YEAH!!! https://t.co/wj3n...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>narendramodi</td>\n",
       "      <td>India has millennia old relations with Oman. W...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jaden</td>\n",
       "      <td>Anxious Teenagers</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JustinTrudeau</td>\n",
       "      <td>Our top priority is keeping Canadians safe. Wi...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>imranyebot</td>\n",
       "      <td>nah bro You’re taking sis so much I’m just a g...</td>\n",
       "      <td>bot</td>\n",
       "      <td>others</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     screen_name                                               text  \\\n",
       "0    ahadsheriff  TIGHT, TIGHT, TIGHT, YEAH!!! https://t.co/wj3n...   \n",
       "1   narendramodi  India has millennia old relations with Oman. W...   \n",
       "2          jaden                                  Anxious Teenagers   \n",
       "3  JustinTrudeau  Our top priority is keeping Canadians safe. Wi...   \n",
       "4     imranyebot  nah bro You’re taking sis so much I’m just a g...   \n",
       "\n",
       "  account.type class_type  labels  \n",
       "0        human      human       0  \n",
       "1        human      human       0  \n",
       "2        human      human       0  \n",
       "3        human      human       0  \n",
       "4          bot     others       1  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweep_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human     1150\n",
       "others     436\n",
       "rnn        370\n",
       "gpt2       346\n",
       "Name: class_type, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweep_val['class_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweep_val['labels'] = tweep_val.apply(lambda x: 0 if x['account.type']=='human' else 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     screen_name                                               text  \\\n",
      "0    ahadsheriff  TIGHT, TIGHT, TIGHT, YEAH!!! https://t.co/wj3n...   \n",
      "1   narendramodi  India has millennia old relations with Oman. W...   \n",
      "2          jaden                                  Anxious Teenagers   \n",
      "3  JustinTrudeau  Our top priority is keeping Canadians safe. Wi...   \n",
      "9         zawvrk                  @jpkayy yo this is not a bad idea   \n",
      "\n",
      "  account.type class_type  labels  \n",
      "0        human      human       0  \n",
      "1        human      human       0  \n",
      "2        human      human       0  \n",
      "3        human      human       0  \n",
      "9        human      human       0  \n"
     ]
    }
   ],
   "source": [
    "tweep_val_human = tweep_val[tweep_val['class_type'] == 'human'].iloc[0:346]\n",
    "print(tweep_val_human.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweep_val_gpt = tweep_val[tweep_val['class_type'] == 'gpt2']\n",
    "tweep_val_gpt = pd.concat([tweep_val_gpt,tweep_val_human])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweep_val_labels = tweep_val_gpt['labels'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    346\n",
       "0    346\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweep_val_gpt['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweep_val_texts = tweep_val_gpt['text'].to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweep_val_encodings = tokenizer(tweep_val_texts, truncation=True, padding=True, max_length=100, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 349ms/step - loss: 3.3567 - accuracy: 0.5029 - precision_2: 0.5015 - recall_2: 0.9971\n"
     ]
    }
   ],
   "source": [
    "results = bert_model.evaluate([tweep_val_encodings.input_ids, tweep_val_encodings.attention_mask], tweep_val_labels, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 3s 131ms/step\n"
     ]
    }
   ],
   "source": [
    "tweep_gpt_predictions = bert_model.predict([tweep_val_encodings.input_ids, tweep_val_encodings.attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.5134312e-01],\n",
       "       [9.9608564e-01],\n",
       "       [9.5159858e-01],\n",
       "       [9.9998069e-01],\n",
       "       [9.9998546e-01],\n",
       "       [9.5663303e-01],\n",
       "       [9.5947099e-01],\n",
       "       [9.9996650e-01],\n",
       "       [9.9998450e-01],\n",
       "       [9.9998331e-01],\n",
       "       [9.9998367e-01],\n",
       "       [9.9997365e-01],\n",
       "       [9.4586021e-01],\n",
       "       [9.6420127e-01],\n",
       "       [9.3531227e-01],\n",
       "       [9.1508782e-01],\n",
       "       [9.9705637e-01],\n",
       "       [9.6581626e-01],\n",
       "       [9.9083602e-01],\n",
       "       [9.8378122e-01],\n",
       "       [9.9910033e-01],\n",
       "       [9.9279988e-01],\n",
       "       [9.9202067e-01],\n",
       "       [9.4405621e-01],\n",
       "       [9.9997127e-01],\n",
       "       [9.9762183e-01],\n",
       "       [9.7715306e-01],\n",
       "       [9.7650015e-01],\n",
       "       [9.7080022e-01],\n",
       "       [9.9968672e-01],\n",
       "       [9.9997270e-01],\n",
       "       [9.9350172e-01],\n",
       "       [9.6598190e-01],\n",
       "       [9.9986756e-01],\n",
       "       [9.9990284e-01],\n",
       "       [9.9997866e-01],\n",
       "       [9.9341398e-01],\n",
       "       [9.9997592e-01],\n",
       "       [9.9997771e-01],\n",
       "       [9.9986339e-01],\n",
       "       [9.9998415e-01],\n",
       "       [9.5121485e-01],\n",
       "       [9.7119313e-01],\n",
       "       [9.9993730e-01],\n",
       "       [9.7537792e-01],\n",
       "       [9.7094923e-01],\n",
       "       [9.9885094e-01],\n",
       "       [9.2398292e-01],\n",
       "       [9.9997258e-01],\n",
       "       [9.6522462e-01],\n",
       "       [9.9995506e-01],\n",
       "       [9.9991131e-01],\n",
       "       [9.9996316e-01],\n",
       "       [9.9997401e-01],\n",
       "       [9.9996793e-01],\n",
       "       [9.2403698e-01],\n",
       "       [9.9998426e-01],\n",
       "       [9.6136051e-01],\n",
       "       [9.8957145e-01],\n",
       "       [9.9995756e-01],\n",
       "       [9.9970132e-01],\n",
       "       [9.6345431e-01],\n",
       "       [9.5619351e-01],\n",
       "       [9.9997306e-01],\n",
       "       [9.7854817e-01],\n",
       "       [9.9997771e-01],\n",
       "       [9.5591497e-01],\n",
       "       [9.9996531e-01],\n",
       "       [9.8674238e-01],\n",
       "       [9.7200012e-01],\n",
       "       [9.9986732e-01],\n",
       "       [9.9480373e-01],\n",
       "       [9.7979659e-01],\n",
       "       [9.9998033e-01],\n",
       "       [9.9998605e-01],\n",
       "       [9.8226261e-01],\n",
       "       [9.7624570e-01],\n",
       "       [9.9968433e-01],\n",
       "       [9.5277649e-01],\n",
       "       [9.7052884e-01],\n",
       "       [9.8211414e-01],\n",
       "       [9.9990726e-01],\n",
       "       [9.9814260e-01],\n",
       "       [9.9998140e-01],\n",
       "       [9.8863399e-01],\n",
       "       [9.7102261e-01],\n",
       "       [9.9998093e-01],\n",
       "       [9.6638161e-01],\n",
       "       [9.9730992e-01],\n",
       "       [9.9943727e-01],\n",
       "       [9.9988449e-01],\n",
       "       [9.9996388e-01],\n",
       "       [9.8618144e-01],\n",
       "       [9.4896740e-01],\n",
       "       [9.2511398e-01],\n",
       "       [9.9998116e-01],\n",
       "       [9.9965787e-01],\n",
       "       [9.2463410e-01],\n",
       "       [9.9997890e-01],\n",
       "       [9.8176193e-01],\n",
       "       [9.9998295e-01],\n",
       "       [9.9995625e-01],\n",
       "       [9.3303013e-01],\n",
       "       [9.8759043e-01],\n",
       "       [9.9997151e-01],\n",
       "       [9.6849424e-01],\n",
       "       [9.1504037e-01],\n",
       "       [9.9998450e-01],\n",
       "       [9.9997735e-01],\n",
       "       [9.9724603e-01],\n",
       "       [9.9973243e-01],\n",
       "       [9.9996173e-01],\n",
       "       [9.9996364e-01],\n",
       "       [9.9997783e-01],\n",
       "       [9.9950802e-01],\n",
       "       [9.3887544e-01],\n",
       "       [9.2395014e-01],\n",
       "       [9.7778499e-01],\n",
       "       [9.9980921e-01],\n",
       "       [9.5585585e-01],\n",
       "       [9.8319489e-01],\n",
       "       [9.8589522e-01],\n",
       "       [9.9995744e-01],\n",
       "       [9.6291351e-01],\n",
       "       [9.9791068e-01],\n",
       "       [9.9527752e-01],\n",
       "       [9.9996376e-01],\n",
       "       [9.9997663e-01],\n",
       "       [9.9967122e-01],\n",
       "       [9.8114741e-01],\n",
       "       [9.9997151e-01],\n",
       "       [9.9996769e-01],\n",
       "       [9.9809736e-01],\n",
       "       [9.8117977e-01],\n",
       "       [9.2808074e-01],\n",
       "       [9.9857104e-01],\n",
       "       [9.9898762e-01],\n",
       "       [9.9994135e-01],\n",
       "       [9.6912843e-01],\n",
       "       [9.9998403e-01],\n",
       "       [9.9997675e-01],\n",
       "       [9.7922164e-01],\n",
       "       [9.4213670e-01],\n",
       "       [9.9996650e-01],\n",
       "       [9.9995279e-01],\n",
       "       [9.6183461e-01],\n",
       "       [9.9993658e-01],\n",
       "       [9.9996185e-01],\n",
       "       [9.9996686e-01],\n",
       "       [9.9498749e-01],\n",
       "       [9.2796695e-01],\n",
       "       [9.9996579e-01],\n",
       "       [9.5862007e-01],\n",
       "       [9.8133379e-01],\n",
       "       [9.9675912e-01],\n",
       "       [9.9890745e-01],\n",
       "       [9.9998164e-01],\n",
       "       [9.9996912e-01],\n",
       "       [9.6256745e-01],\n",
       "       [9.9990642e-01],\n",
       "       [9.7181803e-01],\n",
       "       [9.9995446e-01],\n",
       "       [9.9991751e-01],\n",
       "       [1.2986122e-02],\n",
       "       [9.8064971e-01],\n",
       "       [9.9997032e-01],\n",
       "       [9.9998450e-01],\n",
       "       [9.9997032e-01],\n",
       "       [9.3514448e-01],\n",
       "       [9.9997270e-01],\n",
       "       [9.7415221e-01],\n",
       "       [9.5558506e-01],\n",
       "       [9.9997723e-01],\n",
       "       [9.7478729e-01],\n",
       "       [9.2925328e-01],\n",
       "       [9.9997854e-01],\n",
       "       [9.9990344e-01],\n",
       "       [9.4286638e-01],\n",
       "       [9.9997962e-01],\n",
       "       [9.9997604e-01],\n",
       "       [9.6775961e-01],\n",
       "       [9.9996781e-01],\n",
       "       [9.9997878e-01],\n",
       "       [9.9998391e-01],\n",
       "       [9.8124725e-01],\n",
       "       [9.9920839e-01],\n",
       "       [9.9994957e-01],\n",
       "       [9.9173826e-01],\n",
       "       [9.9996865e-01],\n",
       "       [9.8726267e-01],\n",
       "       [9.9997616e-01],\n",
       "       [9.7226024e-01],\n",
       "       [9.9996448e-01],\n",
       "       [9.9997497e-01],\n",
       "       [9.7050530e-01],\n",
       "       [9.9997771e-01],\n",
       "       [9.9997997e-01],\n",
       "       [9.9997044e-01],\n",
       "       [9.9995351e-01],\n",
       "       [9.9992085e-01],\n",
       "       [9.9997604e-01],\n",
       "       [9.9997211e-01],\n",
       "       [9.4668692e-01],\n",
       "       [9.2318475e-01],\n",
       "       [9.9997640e-01],\n",
       "       [9.7545189e-01],\n",
       "       [9.9998188e-01],\n",
       "       [9.6761316e-01],\n",
       "       [9.9998713e-01],\n",
       "       [9.9995279e-01],\n",
       "       [9.9968469e-01],\n",
       "       [9.7891861e-01],\n",
       "       [9.9997652e-01],\n",
       "       [9.9980062e-01],\n",
       "       [9.9993062e-01],\n",
       "       [9.9994862e-01],\n",
       "       [9.9995065e-01],\n",
       "       [9.9870515e-01],\n",
       "       [9.9754971e-01],\n",
       "       [9.8798358e-01],\n",
       "       [9.9994850e-01],\n",
       "       [9.9997604e-01],\n",
       "       [9.9892199e-01],\n",
       "       [9.9991000e-01],\n",
       "       [9.9571556e-01],\n",
       "       [9.7571725e-01],\n",
       "       [9.9995041e-01],\n",
       "       [9.0782988e-01],\n",
       "       [9.9991596e-01],\n",
       "       [9.5003968e-01],\n",
       "       [9.9996829e-01],\n",
       "       [9.9963284e-01],\n",
       "       [9.9995327e-01],\n",
       "       [9.9994838e-01],\n",
       "       [9.9998403e-01],\n",
       "       [9.9987972e-01],\n",
       "       [9.9992180e-01],\n",
       "       [9.9985528e-01],\n",
       "       [9.9996758e-01],\n",
       "       [9.6875942e-01],\n",
       "       [9.6744752e-01],\n",
       "       [9.8501855e-01],\n",
       "       [9.9992597e-01],\n",
       "       [9.9997950e-01],\n",
       "       [9.9937314e-01],\n",
       "       [9.9989974e-01],\n",
       "       [9.4868612e-01],\n",
       "       [9.9997389e-01],\n",
       "       [9.5726383e-01],\n",
       "       [9.9997818e-01],\n",
       "       [9.9997115e-01],\n",
       "       [9.6912509e-01],\n",
       "       [9.6394557e-01],\n",
       "       [9.9998462e-01],\n",
       "       [9.9982089e-01],\n",
       "       [9.9997354e-01],\n",
       "       [9.9998379e-01],\n",
       "       [9.5376128e-01],\n",
       "       [9.4841957e-01],\n",
       "       [9.9998116e-01],\n",
       "       [9.9997532e-01],\n",
       "       [9.9993992e-01],\n",
       "       [9.9998391e-01],\n",
       "       [9.9995685e-01],\n",
       "       [9.2311531e-01],\n",
       "       [9.9978727e-01],\n",
       "       [9.9983704e-01],\n",
       "       [9.9884063e-01],\n",
       "       [9.9986422e-01],\n",
       "       [9.7723085e-01],\n",
       "       [9.5772630e-01],\n",
       "       [9.9977976e-01],\n",
       "       [9.7646165e-01],\n",
       "       [9.9976426e-01],\n",
       "       [9.6606576e-01],\n",
       "       [9.2697740e-01],\n",
       "       [9.0606892e-01],\n",
       "       [9.9986148e-01],\n",
       "       [9.9677593e-01],\n",
       "       [9.9998653e-01],\n",
       "       [9.8111665e-01],\n",
       "       [9.5530581e-01],\n",
       "       [9.3142009e-01],\n",
       "       [9.9994087e-01],\n",
       "       [9.3444228e-01],\n",
       "       [9.6428961e-01],\n",
       "       [9.4569969e-01],\n",
       "       [9.9498463e-01],\n",
       "       [9.9997950e-01],\n",
       "       [9.9997079e-01],\n",
       "       [9.9985385e-01],\n",
       "       [9.9952543e-01],\n",
       "       [9.9996531e-01],\n",
       "       [9.9997663e-01],\n",
       "       [9.9998558e-01],\n",
       "       [9.7822362e-01],\n",
       "       [9.9857306e-01],\n",
       "       [9.7984719e-01],\n",
       "       [9.9997318e-01],\n",
       "       [9.9478072e-01],\n",
       "       [9.8511988e-01],\n",
       "       [9.9996114e-01],\n",
       "       [9.9993134e-01],\n",
       "       [9.6151048e-01],\n",
       "       [9.9998021e-01],\n",
       "       [9.9998033e-01],\n",
       "       [9.5217234e-01],\n",
       "       [9.6401888e-01],\n",
       "       [9.6701795e-01],\n",
       "       [9.9997795e-01],\n",
       "       [9.9990046e-01],\n",
       "       [9.9997497e-01],\n",
       "       [9.2974806e-01],\n",
       "       [9.9996793e-01],\n",
       "       [9.7356355e-01],\n",
       "       [9.8979670e-01],\n",
       "       [9.3596071e-01],\n",
       "       [9.9996972e-01],\n",
       "       [9.6214950e-01],\n",
       "       [9.8965198e-01],\n",
       "       [9.5030075e-01],\n",
       "       [9.9996793e-01],\n",
       "       [9.5867252e-01],\n",
       "       [9.9995673e-01],\n",
       "       [9.9979097e-01],\n",
       "       [9.9926049e-01],\n",
       "       [8.0819005e-01],\n",
       "       [9.9918801e-01],\n",
       "       [9.9414647e-01],\n",
       "       [9.1963363e-01],\n",
       "       [9.8525780e-01],\n",
       "       [9.9995697e-01],\n",
       "       [9.5518637e-01],\n",
       "       [9.7269529e-01],\n",
       "       [9.6479422e-01],\n",
       "       [9.4507790e-01],\n",
       "       [9.9952483e-01],\n",
       "       [9.9987864e-01],\n",
       "       [9.5830256e-01],\n",
       "       [9.8038656e-01],\n",
       "       [9.7228259e-01],\n",
       "       [9.9997604e-01],\n",
       "       [9.9705708e-01],\n",
       "       [9.9996650e-01],\n",
       "       [9.9995863e-01],\n",
       "       [9.9996400e-01],\n",
       "       [9.9992073e-01],\n",
       "       [9.9996352e-01],\n",
       "       [8.9115757e-01],\n",
       "       [9.9997377e-01],\n",
       "       [9.6036506e-01],\n",
       "       [9.9992537e-01],\n",
       "       [9.8905003e-01],\n",
       "       [9.7710824e-01],\n",
       "       [9.9997902e-01],\n",
       "       [9.9997580e-01],\n",
       "       [9.8032355e-01],\n",
       "       [9.5652354e-01],\n",
       "       [9.9927753e-01],\n",
       "       [9.6981782e-01],\n",
       "       [9.8483825e-01],\n",
       "       [9.8554426e-01],\n",
       "       [9.9996638e-01],\n",
       "       [9.9444795e-01],\n",
       "       [9.9370265e-01],\n",
       "       [9.9539387e-01],\n",
       "       [9.6934950e-01],\n",
       "       [9.7451049e-01],\n",
       "       [9.9147707e-01],\n",
       "       [9.9253392e-01],\n",
       "       [9.9997139e-01],\n",
       "       [9.5839763e-01],\n",
       "       [9.8382020e-01],\n",
       "       [9.9957913e-01],\n",
       "       [9.5760280e-01],\n",
       "       [9.9303257e-01],\n",
       "       [9.7106957e-01],\n",
       "       [9.9996293e-01],\n",
       "       [9.9995577e-01],\n",
       "       [9.7094709e-01],\n",
       "       [9.9583936e-01],\n",
       "       [9.9996340e-01],\n",
       "       [9.9997807e-01],\n",
       "       [9.9643421e-01],\n",
       "       [9.9603647e-01],\n",
       "       [9.9269331e-01],\n",
       "       [9.9669671e-01],\n",
       "       [9.9199623e-01],\n",
       "       [9.9996173e-01],\n",
       "       [9.9997842e-01],\n",
       "       [9.9998224e-01],\n",
       "       [9.9997532e-01],\n",
       "       [9.9478322e-01],\n",
       "       [9.9928063e-01],\n",
       "       [9.6166098e-01],\n",
       "       [9.9997771e-01],\n",
       "       [9.5867240e-01],\n",
       "       [9.8993158e-01],\n",
       "       [9.9997890e-01],\n",
       "       [9.8984104e-01],\n",
       "       [9.9996686e-01],\n",
       "       [9.8255169e-01],\n",
       "       [9.9992335e-01],\n",
       "       [9.9870789e-01],\n",
       "       [9.9997437e-01],\n",
       "       [9.9477404e-01],\n",
       "       [9.9111873e-01],\n",
       "       [9.9069971e-01],\n",
       "       [9.8351687e-01],\n",
       "       [7.7606782e-02],\n",
       "       [9.9604970e-01],\n",
       "       [9.8154700e-01],\n",
       "       [9.9976605e-01],\n",
       "       [9.9339044e-01],\n",
       "       [9.9997938e-01],\n",
       "       [9.9996805e-01],\n",
       "       [9.8955393e-01],\n",
       "       [9.9891067e-01],\n",
       "       [9.9137563e-01],\n",
       "       [9.9995220e-01],\n",
       "       [9.9995852e-01],\n",
       "       [9.9997997e-01],\n",
       "       [9.9582517e-01],\n",
       "       [9.6903986e-01],\n",
       "       [9.9458885e-01],\n",
       "       [9.9747318e-01],\n",
       "       [9.9996650e-01],\n",
       "       [9.8437160e-01],\n",
       "       [9.9994862e-01],\n",
       "       [9.9984324e-01],\n",
       "       [9.2882901e-01],\n",
       "       [9.8990202e-01],\n",
       "       [9.9608827e-01],\n",
       "       [9.9435645e-01],\n",
       "       [9.9998248e-01],\n",
       "       [9.9778217e-01],\n",
       "       [9.7077268e-01],\n",
       "       [9.7904187e-01],\n",
       "       [9.9997568e-01],\n",
       "       [9.8601496e-01],\n",
       "       [9.9811929e-01],\n",
       "       [9.9273235e-01],\n",
       "       [9.8368353e-01],\n",
       "       [9.9995267e-01],\n",
       "       [9.8737001e-01],\n",
       "       [9.9996853e-01],\n",
       "       [9.9530494e-01],\n",
       "       [9.9983180e-01],\n",
       "       [9.7495407e-01],\n",
       "       [9.9101627e-01],\n",
       "       [9.7848171e-01],\n",
       "       [9.9998200e-01],\n",
       "       [9.6739322e-01],\n",
       "       [9.5466673e-01],\n",
       "       [9.6182865e-01],\n",
       "       [9.9993777e-01],\n",
       "       [9.9997616e-01],\n",
       "       [9.9993253e-01],\n",
       "       [9.9686384e-01],\n",
       "       [9.7516906e-01],\n",
       "       [9.9426794e-01],\n",
       "       [9.9735439e-01],\n",
       "       [9.9490851e-01],\n",
       "       [9.7033328e-01],\n",
       "       [9.9913436e-01],\n",
       "       [9.9993539e-01],\n",
       "       [9.3877029e-01],\n",
       "       [9.8977226e-01],\n",
       "       [9.9997365e-01],\n",
       "       [9.8782933e-01],\n",
       "       [9.9733114e-01],\n",
       "       [9.9996305e-01],\n",
       "       [9.8796600e-01],\n",
       "       [9.8878390e-01],\n",
       "       [9.6823084e-01],\n",
       "       [9.9220699e-01],\n",
       "       [9.9997330e-01],\n",
       "       [9.8811662e-01],\n",
       "       [9.9993718e-01],\n",
       "       [9.9996030e-01],\n",
       "       [9.8178089e-01],\n",
       "       [9.9203992e-01],\n",
       "       [9.9975401e-01],\n",
       "       [9.8019248e-01],\n",
       "       [9.8670548e-01],\n",
       "       [9.9997699e-01],\n",
       "       [9.9578947e-01],\n",
       "       [9.9997926e-01],\n",
       "       [9.9163085e-01],\n",
       "       [9.5149159e-01],\n",
       "       [9.9325198e-01],\n",
       "       [9.8726940e-01],\n",
       "       [9.9997127e-01],\n",
       "       [9.8106122e-01],\n",
       "       [9.9698973e-01],\n",
       "       [9.9989831e-01],\n",
       "       [9.9997878e-01],\n",
       "       [9.6620142e-01],\n",
       "       [9.9227941e-01],\n",
       "       [9.9996543e-01],\n",
       "       [9.9997234e-01],\n",
       "       [9.9986017e-01],\n",
       "       [9.9998105e-01],\n",
       "       [9.6053725e-01],\n",
       "       [9.9834168e-01],\n",
       "       [9.9978930e-01],\n",
       "       [1.2921778e-04],\n",
       "       [9.6458191e-01],\n",
       "       [9.9997294e-01],\n",
       "       [9.3443501e-01],\n",
       "       [9.9764365e-01],\n",
       "       [9.9996746e-01],\n",
       "       [9.9997437e-01],\n",
       "       [9.8942250e-01],\n",
       "       [9.1362154e-01],\n",
       "       [9.9255812e-01],\n",
       "       [9.9998307e-01],\n",
       "       [9.8582709e-01],\n",
       "       [9.9997425e-01],\n",
       "       [9.9997926e-01],\n",
       "       [9.9998641e-01],\n",
       "       [9.9932098e-01],\n",
       "       [9.9734914e-01],\n",
       "       [9.9801731e-01],\n",
       "       [9.8675519e-01],\n",
       "       [9.4773948e-01],\n",
       "       [9.9395692e-01],\n",
       "       [9.8908395e-01],\n",
       "       [9.9998331e-01],\n",
       "       [9.9729341e-01],\n",
       "       [9.8815644e-01],\n",
       "       [9.9998116e-01],\n",
       "       [9.9997067e-01],\n",
       "       [9.9997127e-01],\n",
       "       [9.8734426e-01],\n",
       "       [9.9995816e-01],\n",
       "       [9.9875736e-01],\n",
       "       [9.9942422e-01],\n",
       "       [9.9996912e-01],\n",
       "       [9.9997282e-01],\n",
       "       [9.9178809e-01],\n",
       "       [9.8640519e-01],\n",
       "       [9.9989617e-01],\n",
       "       [9.9997973e-01],\n",
       "       [9.9995744e-01],\n",
       "       [9.8819387e-01],\n",
       "       [9.9997842e-01],\n",
       "       [9.3329597e-01],\n",
       "       [9.9993587e-01],\n",
       "       [9.9998248e-01],\n",
       "       [9.9998069e-01],\n",
       "       [9.7687507e-01],\n",
       "       [9.9431229e-01],\n",
       "       [9.8933828e-01],\n",
       "       [9.9343854e-01],\n",
       "       [9.9996734e-01],\n",
       "       [9.9997747e-01],\n",
       "       [9.9995399e-01],\n",
       "       [9.9260312e-01],\n",
       "       [9.9038482e-01],\n",
       "       [9.9011552e-01],\n",
       "       [9.7714657e-01],\n",
       "       [9.9997985e-01],\n",
       "       [9.9098295e-01],\n",
       "       [9.9738675e-01],\n",
       "       [9.9998176e-01],\n",
       "       [9.8980546e-01],\n",
       "       [9.9343997e-01],\n",
       "       [9.9126399e-01],\n",
       "       [9.6030909e-01],\n",
       "       [9.9997759e-01],\n",
       "       [9.9926943e-01],\n",
       "       [9.9332786e-01],\n",
       "       [9.8879284e-01],\n",
       "       [9.9995995e-01],\n",
       "       [9.9997604e-01],\n",
       "       [9.7626889e-01],\n",
       "       [9.7271425e-01],\n",
       "       [9.8853707e-01],\n",
       "       [9.9995255e-01],\n",
       "       [9.9998307e-01],\n",
       "       [9.9998140e-01],\n",
       "       [9.9997509e-01],\n",
       "       [9.7954798e-01],\n",
       "       [9.9997962e-01],\n",
       "       [9.9087244e-01],\n",
       "       [9.9996150e-01],\n",
       "       [9.6404737e-01],\n",
       "       [9.7902566e-01],\n",
       "       [9.9461466e-01],\n",
       "       [9.8478705e-01],\n",
       "       [9.9609560e-01],\n",
       "       [9.4653088e-01],\n",
       "       [9.9940920e-01],\n",
       "       [9.9997735e-01],\n",
       "       [9.7812384e-01],\n",
       "       [9.9997199e-01],\n",
       "       [9.9997139e-01],\n",
       "       [9.7909933e-01],\n",
       "       [9.9997485e-01],\n",
       "       [9.9132478e-01],\n",
       "       [9.6892691e-01],\n",
       "       [7.0099486e-05],\n",
       "       [9.9987960e-01],\n",
       "       [9.9998391e-01],\n",
       "       [9.9221331e-01],\n",
       "       [9.9995780e-01],\n",
       "       [9.9998188e-01],\n",
       "       [9.8240721e-01],\n",
       "       [9.8527157e-01],\n",
       "       [9.9998164e-01],\n",
       "       [9.9007320e-01],\n",
       "       [9.9421304e-01],\n",
       "       [9.6018785e-01],\n",
       "       [9.9786502e-01],\n",
       "       [9.9997902e-01],\n",
       "       [9.9997628e-01],\n",
       "       [9.8738343e-01],\n",
       "       [9.9995971e-01],\n",
       "       [9.6052933e-01],\n",
       "       [9.9198794e-01],\n",
       "       [9.9945134e-01],\n",
       "       [9.9569452e-01],\n",
       "       [9.9772567e-01],\n",
       "       [9.9559599e-01],\n",
       "       [9.9997294e-01],\n",
       "       [9.9995399e-01],\n",
       "       [9.9996865e-01],\n",
       "       [9.8240870e-01],\n",
       "       [9.9293423e-01],\n",
       "       [9.9604875e-01],\n",
       "       [9.8722780e-01],\n",
       "       [9.9998009e-01],\n",
       "       [9.9073339e-01],\n",
       "       [9.9995255e-01],\n",
       "       [9.9652171e-01],\n",
       "       [9.6597803e-01],\n",
       "       [9.9986994e-01],\n",
       "       [9.8171723e-01],\n",
       "       [9.9998093e-01],\n",
       "       [9.9992597e-01],\n",
       "       [9.8230976e-01],\n",
       "       [9.7341162e-01],\n",
       "       [9.9978465e-01],\n",
       "       [9.9232578e-01],\n",
       "       [9.9994957e-01],\n",
       "       [9.9997127e-01],\n",
       "       [9.9915075e-01],\n",
       "       [9.9331999e-01],\n",
       "       [9.9998224e-01],\n",
       "       [9.9998379e-01],\n",
       "       [9.8818564e-01],\n",
       "       [9.8660129e-01],\n",
       "       [9.9991930e-01],\n",
       "       [9.5969790e-01],\n",
       "       [9.5218122e-01],\n",
       "       [9.9073571e-01],\n",
       "       [9.9998140e-01],\n",
       "       [9.7015882e-01],\n",
       "       [9.7494441e-01],\n",
       "       [9.9998403e-01],\n",
       "       [9.2900729e-01],\n",
       "       [9.8922336e-01],\n",
       "       [9.9997807e-01],\n",
       "       [9.6521729e-01],\n",
       "       [9.4118059e-01],\n",
       "       [9.9995601e-01],\n",
       "       [9.8725706e-01],\n",
       "       [9.4290829e-01],\n",
       "       [9.9350834e-01],\n",
       "       [9.9064142e-01],\n",
       "       [9.9553645e-01],\n",
       "       [9.9517715e-01],\n",
       "       [9.9997640e-01],\n",
       "       [9.8881173e-01],\n",
       "       [9.5638669e-01],\n",
       "       [9.4087309e-01],\n",
       "       [9.3158603e-01],\n",
       "       [9.9998271e-01],\n",
       "       [9.9008209e-01],\n",
       "       [9.8736972e-01],\n",
       "       [9.9281609e-01],\n",
       "       [9.9996626e-01],\n",
       "       [9.9754250e-01],\n",
       "       [9.9998212e-01],\n",
       "       [9.9219632e-01],\n",
       "       [9.9997795e-01],\n",
       "       [9.8491734e-01],\n",
       "       [9.9997258e-01],\n",
       "       [9.9997103e-01],\n",
       "       [9.9998319e-01],\n",
       "       [9.9929059e-01]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweep_gpt_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427/427 [==============================] - 57s 132ms/step\n"
     ]
    }
   ],
   "source": [
    "#bert_pred_encode = tokenizer(train_texts[4702:4802], truncation=True, padding=True, max_length=100, return_tensors='tf')\n",
    "\n",
    "bert_predict = bert_model.predict([train_encodings.input_ids, train_encodings.attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.2755860e-06],\n",
       "       [2.7211217e-06],\n",
       "       [1.8595711e-05],\n",
       "       [3.2195430e-06],\n",
       "       [3.1999066e-06],\n",
       "       [3.5889225e-06],\n",
       "       [2.8994734e-06],\n",
       "       [3.9225833e-06],\n",
       "       [3.8584158e-06],\n",
       "       [3.4489824e-06],\n",
       "       [3.6142160e-06],\n",
       "       [5.4012035e-06],\n",
       "       [3.1920856e-06],\n",
       "       [3.3419155e-06],\n",
       "       [3.5553949e-06],\n",
       "       [3.5484945e-06],\n",
       "       [2.5294612e-06],\n",
       "       [3.8700609e-06],\n",
       "       [3.8703415e-06],\n",
       "       [3.5436822e-06],\n",
       "       [5.3957051e-06],\n",
       "       [2.8228801e-06],\n",
       "       [2.8636939e-06],\n",
       "       [2.7957940e-06],\n",
       "       [2.7072244e-06],\n",
       "       [2.8786937e-06],\n",
       "       [3.4626466e-06],\n",
       "       [3.1666680e-06],\n",
       "       [3.5914666e-06],\n",
       "       [2.6139069e-06],\n",
       "       [2.5625941e-06],\n",
       "       [2.4315711e-06],\n",
       "       [3.0796136e-06],\n",
       "       [3.2618336e-06],\n",
       "       [2.7170145e-06],\n",
       "       [2.8772226e-06],\n",
       "       [2.7359017e-06],\n",
       "       [2.7387134e-06],\n",
       "       [1.9781797e-05],\n",
       "       [2.6770042e-06],\n",
       "       [3.1006309e-06],\n",
       "       [3.0759711e-06],\n",
       "       [4.5178481e-06],\n",
       "       [2.6931723e-06],\n",
       "       [5.4745310e-06],\n",
       "       [3.6520860e-06],\n",
       "       [4.3537980e-06],\n",
       "       [2.5223167e-06],\n",
       "       [2.5029501e-06],\n",
       "       [3.5943999e-06],\n",
       "       [3.3709769e-06],\n",
       "       [2.7209505e-06],\n",
       "       [9.9999094e-01],\n",
       "       [9.9999213e-01],\n",
       "       [9.9999130e-01],\n",
       "       [9.9999249e-01],\n",
       "       [9.9998832e-01],\n",
       "       [9.9999201e-01],\n",
       "       [9.9999166e-01],\n",
       "       [9.9998951e-01],\n",
       "       [9.9998975e-01],\n",
       "       [9.9999142e-01],\n",
       "       [9.9999154e-01],\n",
       "       [9.9999166e-01],\n",
       "       [9.9999034e-01],\n",
       "       [9.9999118e-01],\n",
       "       [9.9999225e-01],\n",
       "       [9.9999106e-01],\n",
       "       [9.9999082e-01],\n",
       "       [9.9999118e-01],\n",
       "       [9.9999034e-01],\n",
       "       [9.9998558e-01],\n",
       "       [9.9999154e-01],\n",
       "       [9.9998915e-01],\n",
       "       [9.9998641e-01],\n",
       "       [9.9999106e-01],\n",
       "       [9.9999166e-01],\n",
       "       [9.9999213e-01],\n",
       "       [9.9999094e-01],\n",
       "       [9.9999166e-01],\n",
       "       [9.9999070e-01],\n",
       "       [9.9999058e-01],\n",
       "       [9.9998796e-01],\n",
       "       [9.9999225e-01],\n",
       "       [9.9999225e-01],\n",
       "       [9.9999213e-01],\n",
       "       [9.9999249e-01],\n",
       "       [9.9999142e-01],\n",
       "       [9.9998772e-01],\n",
       "       [9.9999034e-01],\n",
       "       [9.9999046e-01],\n",
       "       [9.9999154e-01],\n",
       "       [9.9999022e-01],\n",
       "       [9.9999118e-01],\n",
       "       [9.9998927e-01],\n",
       "       [9.9999225e-01],\n",
       "       [9.9998820e-01],\n",
       "       [9.9999213e-01],\n",
       "       [9.9999130e-01],\n",
       "       [9.9999058e-01]], dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_predict[4200:4300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4244 9395\n",
      "13652\n"
     ]
    }
   ],
   "source": [
    "zero = 0\n",
    "one = 0\n",
    "for i in bert_predict:\n",
    "    if i < 0.01:\n",
    "        zero += 1\n",
    "    elif i > 0.99:\n",
    "        one += 1\n",
    "print(zero, one)\n",
    "print(len(bert_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 42s 396ms/step - loss: 9.5534e-04 - accuracy: 0.9997 - precision_2: 0.9996 - recall_2: 1.0000\n"
     ]
    }
   ],
   "source": [
    "bert_train_eval = bert_model.evaluate([train_encodings.input_ids, train_encodings.attention_mask], wiki_train_labels, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "38_ve",
   "language": "python",
   "name": "38_ve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
